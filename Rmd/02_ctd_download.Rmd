---
title: "CTD download from ERDDAP"
author: "Sebastian DiGeronimo"
date: "2023-01-10"
output: html_document
---

# 1.0 ---- Summary of Document ----


Steps:
1. Make sure to download libraries if you don't have
  i.e. install.packages("purrr")
2. Check database URL
3. Change folder location if needed
  - the code will add cruise specific folders within that location and their 
    station files
4. Create folders
5. Run function on cruise IDs


# 2.0 ---- Setup ----

## 2.1 Load libraries
```{r setup}
if (!nzchar(system.file(package = "librarian"))) 
    install.packages("librarian")

librarian::shelf(
    librarian, conflicted, ggplot2, tibble, tidyr, readr, purrr, dplyr, stringr,
    forcats, lubridate, glue, fs, magrittr, here,
    # broom # optional
    
    # additional
    rerddap, cli,
    quiet = TRUE
)

# shelf(conflicted) # may be needed if won't allow loading of certain packages

conflicts_prefer(
    dplyr::filter(), 
    dplyr::select()
    )

source(here("scripts", "misc_functions.R"))
source(here("scripts", "ctd_download.R"))
```

## 2.2 Set database Website

Once you find an ERDDAP with the data you want, you can change the location here

TAMU ERDDAP publishes the CTD data from the SFP/MBON cruises
- "https://gcoos5.geos.tamu.edu/erddap/"

PFEG ERDDAP (not used)
- "https://upwell.pfeg.noaa.gov/erddap/" 
- contains meterological data
  - wanted to use
    - wind speed 
    - Ekman transport

```{r set-database}
# this database contains CTD data from Walton Smith Cruises
"https://gcoos5.geos.tamu.edu/erddap/" %>%
 Sys.setenv(RERDDAP_DEFAULT_URL = .)

```

## 2.3 Query ERDDAP for CTD Data and Compare to Cruise IDs from Pigment Data

Research vessel acronym:
  - WS = R/V Walton Smith (majority)
  - SAV = R/V Savannah
  - WB = R/V Weatherbird II
  - H = 
  
NOTE: some cruise IDs were modified because there is some slight difference in 
      IDs during the same cruise. This is usually due to a delay in cruise start
      date and was not changed prior to departure.
      
      
```{r query-erddap-cruise-id}
# folder locationt to save data
dir_data_save <- here("data", "processed", "ctd")
dir_data_load <- rstudioapi::selectDirectory()

dir_create(dir_data_save)

# cruise IDs covering all pigment data available
cruise_id <-
  here("data", "processed", "load_data", "combined_pig_dat.csv") %>%
  read_csv(show_col_types = FALSE) %>%
  mutate(
    cruise_id = str_extract(sample, "\\w{1,3}\\d{3,5}"),
    cruise_id = case_when(
        str_detect(cruise_id, "SV18067") ~ "SAV1803",
        str_detect(cruise_id, "SV18173") ~ "SAV18173",
        str_detect(cruise_id, "WS20230") ~ "WS20231",
        str_detect(cruise_id, "WS20278") ~ "WS20279",
         .default = cruise_id
    )
    ) %>%
  distinct(cruise_id) %>% 
  pull(cruise_id) %T>%
  print()

# all cruises since 2016
# this covers all possible cruises to download more than neceessary
all_cruise_info <-
  ed_search_adv(
    query     = "Walton Smith",
    minTime   = "2016-01-01T01:00:00Z",
    page_size = 3000
  )

all_cruise_ids <-
  str_extract(all_cruise_info$info$title, "(WS|SAV|WB|H)\\d{3,5}") %>%
  unique()



# ---- print info about cruise IDs found
cli::cli_alert_info(
  c(
    "{col_green(\"Number of files queried\")}: ",
    "{nrow(all_cruise_info$info)} files\n",
    "{col_blue(\"Cruise IDs\")}:\n{all_cruise_ids}\n\n"
  )
)

# shows cruises that don't exists
cli_alert_info(
  c(
    "{col_red(\"Pigment Cruise IDs not in ERDDAP\")}:\n",
    "{cruise_id[!cruise_id %in% all_cruise_ids]}\n\n",
    "Note: this cruise CTD data {col_red(\"does not exists\")}.\n\n"
  )
)

# shows cruises that are not in our cruise_id list between 2016 and current
# NOTES: I prefer this one as it will have extra data if needed
cli_alert_info(
  c(
    "{col_magenta(\"Extra Cruise IDs\")}:\n",
    "{all_cruise_ids[!all_cruise_ids %in% cruise_id]}\n\n",
    "NOTE: These cruises exist for the NOAA/MBON project but do not ",
    "currently have pigment data available.\n\n"
  )
)

```
# 3.0 ---- CTD Data Download ----

## 3.1 Download CTD Data from ERRDAP

This will take some time to get every cruise, ~ 30 - 60 min.

It may be better to filter for cruise IDs with pigment data first and allow to 
run. 

NOTE: if the connection to the server fails, it's most likely they kicked you
      off because of the high request rates. This is to block spammers on the
      system and they don't know that we're downloading data to use.

```{r download-ctd-data}
tictoc::tic()
for (cr_id in seq(all_cruise_ids)) {
  cat(sprintf("%2.2-d of %2.2-d Cruises\n", cr_id, length(all_cruise_ids)))
  try(
    ctd_downloads(
      IDS = all_cruise_ids[cr_id],
      path = dir_data_save,
      verbose = TRUE,
      overwrite = FALSE
    ),
    silent = FALSE
  )
}

tictoc::toc()

rm(cr_id)
```

# 3.2 Extract Cruise ID and Station Name from All Files

NOTE: some cruise IDs were modified because there is some slight difference in 
      IDs during the same cruise. This is usually due to a delay in cruise start
      date and was not changed prior to departure.

Station names are not in a consistent format and here we correct it to match
our naming.

```{r query-ctd-files}
# get all CTD file names
file_ctd <-
  dir_ls(
    path    = dir_data_save,
    recurse = TRUE,
    regexp  = "\\.csv$"
  ) %>%
  str_sort() %>%
  tibble(file = .) %>%
  mutate(
    base = basename(file),
    base = path_ext_remove(base),
    base = str_to_upper(base)
  ) %>%
  separate_wider_delim(base, "_",
    names = c("cruise_id", "stn"),
    too_many = "merge",
    cols_remove = FALSE
  ) %>%
  mutate(
    # fix cruise IDs to match ours
    cruise_id = case_when(
      str_detect(cruise_id, "SAV1803")  ~ "SV18067",
      str_detect(cruise_id, "SAV18173") ~ "SV18173",
      str_detect(cruise_id, "WS20231")  ~ "WS20230",
      str_detect(cruise_id, "WS20279")  ~ "WS20278",
      .default = cruise_id
    ),
    
    # extrat station name and fix to a standard format
    station = str_remove(stn, "(?i)(stn|sta)_?"),
    station = case_when(
      !str_detect(station, "Z\\d{2}") ~
        str_replace(
          station,
          "(\\d)(-|/|_)(\\d)",
          "\\1\\.\\3"
        ),
      .default = station
    ),
    station = str_remove(station, "^0+"),
    station = case_when(
      str_detect(station, "(?i)LK")   ~ "LK",
      str_detect(station, "(?i)^21$") ~ "LK",
      str_detect(station, "21.+5")    ~ "21.5",
      .default = station
    ),
    station = str_replace(station, "-", "_")
  ) %>%
  filter(str_detect(station, "(?i)hex|test", negate = TRUE)) %T>%
  print()
```

# 4.0 ---- Summarize CTD Files ----

## 4.1 Average Top 5 meters

Data variables:
- time     - date time (yyyy-mm-dd HH:MM:SS)
- lat      - latitude  (decimal degrees)
- lon      - longitude (decimal degrees)
- depth    - depth (meter)
- pressure - pressure (dB)
- temp     - temperature (degrees C)
- o_sat    - oxygen saturation (%)
- do       - dissolved oxygen (mg L^-1 ?)
- par      - photosynthetically active radiation ()
- sal      - salinity (unitless)

Uses the function: `read_ctd_csv`.

Two ways of averaging top 5 meters:

1. Using parallel processing `future_pmap()` (faster)
    - requires `furrr`, `future`, and `progressr` 
    - this runs each summarization in parallel using cores within the computure
    - for mine, its ~2 min in parallel and ~6 min in series
    
2. Using normal sequential `pmap()`
    - slower, but is easier to implement
    - if not too many loops, this is preferred
    
## 4.1.1 Averages by Parallel Processing

Will try to run parallel processing and will skip if cannot find the packages

```{r load-ctd-data}
# look for `furrr` and `progressr` package, and will skip is not downloaded.
parallel_pkgs <- c("furrr", "progressr")
if (
  nzchar(system.file(package = parallel_pkgs[1])) &
    nzchar(system.file(package = parallel_pkgs[2]))
) {
  shelf(furrr, progressr)
  handlers("cli")

  tictoc::tic() # <-- start clock
  plan(multisession, workers = 5)

  with_progress({
    p <- progressor(steps = nrow(file_ctd))

    ctd_dat_summary <-
      file_ctd %>%
      mutate(
        data = future_pmap(
          p = p,
          list(file, cruise_id, station),
          read_ctd_csv
        )
      ) %>%
      unnest(cols = data)
  })

  plan(sequential)

  tictoc::toc() # <-- end clock

  unshelf(furrr, future)
} else {
  cli_alert_warning(
    c(
      "The packages {.pkg {parallel_pkgs}} ",
      "{col_red(\"were not found\")} in your search path.\n\n",
      "If you want to do parallel processing, try:\n\n",
      "\t{.code librarian::shelf({parallel_pkgs[1]}, {parallel_pkgs[2]})}"
    )
  )
}

rm(parallel_pkgs)
```

## 4.1.2 Averages by Sequential Processing

Slower method, but should work if the packages for parallel processing are not
available

```{r load-ctd-data}
# If `ctd_dat_summary` was not created, run normal
if (!exists("ctd_dat_summary")) {
  tictoc::tic() # <-- start clock

  ctd_dat_summary <-
    file_ctd %>%
    mutate(
      data = pmap(
        .progress = TRUE,
        list(file, cruise_id, station),
        p = NULL,
        read_ctd_csv
      )
    ) %>%
    unnest(cols = data)

  tictoc::toc() # <-- end clock
}

```

# 5.0 Save

## 5.1 Save 5 meter Averaged CTD Data
```{r save-ctd-all}
# view NAs in the data
naniar::vis_miss(ctd_dat_summary)

ctd_dat_summary %>%
  arrange(time) %>%
  save_csv(
    .data          = .,
    save_location  = dir_data_save,
    save_name      = "ctd_all",
    overwrite      = FALSE,
    verbose        = TRUE,
    time_stamp_fmt = NULL
  )
```


```{r}
ctd_dat_summary %>%
  ggplot(
      aes(x = time,
          y = temp)
  ) +
    geom_point()
ctd_dat_summary %>%
  ggplot(
      aes(x = time,
          y = sal)
  ) +
    geom_point()
```


# TODO: remove to another location
the cmtx_avgs variable doesn't exists here
```{r chemtax-ctd}
if (FALSE) {
    # cmtx_avgs created in chemtax_analysis.Rmd
    cmtx_avgs <- 
        here("data", "processed", "chemtax_w_meta_rm_dup.csv")  %>%
        read_csv(show_col_types = FALSE) %>%
        mutate(cruise_id = str_extract(sample, "[A-Z]{2,3}[0-9]{4,5}"), 
               season    = as.character(season)) %>%
        relocate(cruise_id, .before = 1)
    
    
    ctd_join <-
    here("data", "processed") %>%
    dir_ls(regexp = "ctd_all") %>%
    sort(decreasing = TRUE) %>%
    first() %>%
    read_csv(show_col_types = FALSE) %>%
    mutate(time = as_datetime(time)) %>%
    filter(
        between(time, 
                min(cmtx_avgs$date_time_utc) - period("1 month"),
                max(cmtx_avgs$date_time_utc) + period("1 month"))
        ) 
    
    # join cmtx_avgs with ctd summary
    env_cmtx <-
      cmtx_avgs %>%
      left_join(
        ctd_join,
        by = join_by(
          "cruise_id",
          "station"),
        relationship = "many-to-many"
      ) %>%
      mutate(
        time_diff = abs(difftime(date_time_utc, time.y, units = "mins")) %>%
                    as.numeric(),
        time_diff = if_else(is.na(time_diff), 1, time_diff)
      ) %>%
      filter(
        .by = c(cruise_id, station, depth.x),
        time_diff == min(time_diff)
      ) %>%
      select(-time_diff)

    
    write_csv(
      env_cmtx,
      file = here("data", "processed", 
                   glue("cmtx_env_data_", 
                        "{Sys.Date()}",
                        ".csv")
                  )
    )
    }
```

