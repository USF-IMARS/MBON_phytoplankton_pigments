---
title: "CTD download from ERDDAP"
author: "Sebastian DiGeronimo"
date: "2023-01-10"
output: html_document
---

# Steps:
1. Make sure to download libraries if you don't have
  i.e. install.packages("purrr")
2. Check database URL
3. Change folder location if needed
  - the code will add cruise specific folders within that location and their 
    station files
4. Create folders
5. Run function on cruise IDs


# 1.0 Load libraries
```{r setup}
if (!nzchar(system.file(package = "librarian"))) 
    install.packages("librarian")

librarian::shelf(
    librarian, conflicted, ggplot2, tibble, tidyr, readr, purrr, dplyr, stringr,
    forcats, lubridate, glue, fs, magrittr, here,
    # broom # optional
    
    # additional
    rerddap,
    quiet = TRUE
)

# shelf(conflicted) # may be needed if won't allow loading of certain packages

conflicts_prefer(
    dplyr::filter(), 
    dplyr::select()
    )

source(here("scripts", "misc_functions.R"))
source(here("scripts", "ctd_download.R"))
```

# 2.0 Set database
Once you find an ERDDAP with the data you want, you can change the location here
```{r set-database}
# this database contains CTD data from Walton Smith Cruises
database <- "https://gcoos5.geos.tamu.edu/erddap/"
# database <- "https://upwell.pfeg.noaa.gov/erddap/" 
Sys.setenv(RERDDAP_DEFAULT_URL = database)
rm(database)
```

# 3.0 Set folder location and create vector of cruise IDs
NOTE: WB22215 is WS22215
      - WB = R/V Weatherbird II
      - WS = R/V Walton Smith 
      - this was on Weatherbird
```{r loc-ids}
# folder location
loc      <- here("data", "raw", "ctd")

# cruise IDs
cruiseID <- c("WS16074", "WS16130", "WS16263", "WS16319", "WS17086", "WS17170",
              "WS17282", "WS18008", "SAV1803", "WS18120", "SAV18173", "WS18218",
              "WS18285", "WS18351", "WS19028", "WS19119", "WS19210", "WS19266",
              "WS19322", "WS20006", "WS20231", "WS20278", "WS20279", "WS20342",
              "WS21032", "WS21093")

# all cruises since 2016
all_cruise <- 
    ed_search_adv(
        query     = "Walton Smith",
        minTime   = "2016-01-01T01:00:00Z",
        page_size = 3000) 

num_stns <- length(all_cruise$info$title)

all_cruise <-
    str_extract(all_cruise$info$title, "(WS|SAV|WB|H)\\d{3,5}") %>%
    unique()

# shows cruises that don't exists
cruiseID[!cruiseID %in% all_cruise]

# shows cruises that are not in our cruiseID list between 2016 and current
# NOTES: I prefer this one as it will have extra data if needed
all_cruise[!all_cruise %in% cruiseID]
```
# 4.0 Run ctd_downloads function and try each cruise ID for existing data
This will take some time, ~ 30 - 60 min
```{r download-ctd-data}
tictoc::tic()
for (cr_id in seq(all_cruise)) {
    cat(sprintf("%2.2-d of %2.2-d Cruises\n", cr_id, length(all_cruise)))
    try(ctd_downloads(IDS       = all_cruise[cr_id],
                      path      = loc,
                      verbose   = TRUE,
                      overwrite = FALSE),
        silent = FALSE)
    }

tictoc::toc()

rm(cr_id)
```

# 5.0 Find Path to All CTD Files and Extract Cruise ID and Station Name

```{r query-ctd-files}
# get all CTD file names
file_ctd <-
  dir_ls(loc,
    recurse = TRUE,
    regexp = "\\.csv$"
  ) %>%
  str_sort() %>%
  tibble(file = .) %>%
  mutate(
    base = basename(file) %>%
      path_ext_remove() %>%
      str_to_upper()
  ) %>%
  separate_wider_delim(base, "_",
    names = c("cruise_id", "stn"),
    too_many = "merge",
    cols_remove = FALSE
  ) %>%
  mutate(
    # fix cruise IDs
    cruise_id = case_when(
      str_detect(cruise_id, "SAV18173") ~ "SV18173",
      str_detect(cruise_id, "SAV1803") ~ "SV18067",
      str_detect(cruise_id, "WS20231") ~ "WS20230",
      str_detect(cruise_id, "WS20279") ~ "WS20278",
      .default = cruise_id
    ),
    station = str_remove(stn, "(?i)(stn|sta)_?"),
    station = case_when(
      !str_detect(station, "Z\\d{2}") ~
        str_replace(
          station,
          "(\\d)(-|/|_)(\\d)",
          "\\1\\.\\3"
        ),
      .default = station
    ),
    station = str_remove(station, "^0+"),
    station = case_when(
      str_detect(station, "(?i)LK") ~ "LK",
      str_detect(station, "(?i)^21$") ~ "LK",
      str_detect(station, "21.+5") ~ "21.5",
      .default = station
    ),
    station = str_replace(station, "-", "_")
  )
```

# 6.0 Summarize CTD Files
Two version:
1. Using parallel processing `future_pmap()` (faster)
    - requires `furrr`, `future`, and `progressr` 
    - this runs each summarization in parallel using cores within the computure
    - for mine, its ~2 min in parallel and ~6 min in series
    
2. Using normal sequential `pmap()`
    - slower, but is easier to implement
    - if not too many loops, this is preferred
```{r load-ctd-data}
shelf(furrr, progressr)
handlers("cli")
fn <- function(x, y, z, p) {
    out <- 
        # ---- read file
        read_csv(
            x,
            show_col_types = FALSE,
            name_repair = "unique_quiet")  %>%
        
        # filter depth
        filter(depth <= 5) %>% 
            
        # select cols and rename
        select(
          time, 
          lat      = latitude, 
          lon      = longitude, depth,
          pressure = sea_water_pressure,
          temp     = sea_water_temperature, 
          o_sat    = oxygen_saturation, do = dissolved_oxygen,
          par      = any_of("photosynthetically_available_radiation"), 
          sal      = sea_water_salinity) %>%
    
        # calc average
        summarise(across(.cols = everything(),
                         .fn = \(x) mean(x, na.rm = TRUE)))
    
    p(glue("Cruise {y}, Station: {z}"))
    out
}

tictoc::tic() # <-- start clock
plan(multisession, workers = 5)

with_progress({
    
    p <- progressor(steps = nrow(file_ctd))
    
    ctd_dat_summary <-
    file_ctd %>%
    filter(str_detect(station, "(?i)hex|test", negate = TRUE)) %>%
    mutate(
        data = future_pmap(
            p = p,
            list(file, cruise_id, station),
            fn)) %>%
    unnest(cols = data)
})

plan(sequential)


# If `ctd_dat_summary` was not created, run normal
if (!exists("ctd_dat_summary")) {
    tictoc::tic() # <-- start clock
    
    ctd_dat_summary <-
        file_ctd %>%
        filter(str_detect(station, "(?i)hex|test", negate = TRUE)) %>%
        slice(1:10) %>%
        mutate(
            data = pmap(
                .progress = TRUE,
                
                list(file, cruise_id, station),
                function(x, y, z) {
                cli::cli_alert_info("Cruise {y}, Station: {z}")
                
                out <- read_csv(
                    x,
                    show_col_types = FALSE,
                    name_repair = "unique_quiet")  %>%
                    
                    # filter depth
                    filter(depth <= 5) 
                
                message(paste(names(out), "\n"))
                
                out <-  out %>% 
                        
                    # select cols and rename
                    select(
                      time, 
                      lat      = latitude, 
                      lon      = longitude, depth,
                      pressure = sea_water_pressure,
                      temp     = sea_water_temperature, 
                      o_sat    = oxygen_saturation, do = dissolved_oxygen,
                      par      = any_of("photosynthetically_available_radiation"), 
                      sal      = sea_water_salinity) %>%
                
                    # calc average
                    summarise(across(.cols = everything(),
                                     .fn = \(x) mean(x, na.rm = TRUE)))
                }
                )
            ) %>%
        unnest(cols = data)
}

tictoc::toc() # <-- end clock
unshelf(furrr, future)
rm(fn, p)
```


```{r save-ctd-all}
ctd_dat_summary %>%
  arrange(time) %>%
  save_csv(
    .data          = .,
    save_location  = here("data", "processed"),
    save_name      = "ctd_all",
    overwrite      = FALSE,
    verbose        = TRUE,
    time_stamp_fmt = NULL
  )
```


# TODO: remove to another location
the cmtx_avgs variable doesn't exists here
```{r chemtax-ctd}
if (FALSE) {
    # cmtx_avgs created in chemtax_analysis.Rmd
    cmtx_avgs <- 
        here("data", "processed", "chemtax_w_meta_rm_dup.csv")  %>%
        read_csv(show_col_types = FALSE) %>%
        mutate(cruise_id = str_extract(sample, "[A-Z]{2,3}[0-9]{4,5}"), 
               season    = as.character(season)) %>%
        relocate(cruise_id, .before = 1)
    
    
    ctd_join <-
    here("data", "processed") %>%
    dir_ls(regexp = "ctd_all") %>%
    sort(decreasing = TRUE) %>%
    first() %>%
    read_csv(show_col_types = FALSE) %>%
    mutate(time = as_datetime(time)) %>%
    filter(
        between(time, 
                min(cmtx_avgs$date_time_utc) - period("1 month"),
                max(cmtx_avgs$date_time_utc) + period("1 month"))
        ) 
    
    # join cmtx_avgs with ctd summary
    env_cmtx <-
      cmtx_avgs %>%
      left_join(
        ctd_join,
        by = join_by(
          "cruise_id",
          "station"),
        relationship = "many-to-many"
      ) %>%
      mutate(
        time_diff = abs(difftime(date_time_utc, time.y, units = "mins")) %>%
                    as.numeric(),
        time_diff = if_else(is.na(time_diff), 1, time_diff)
      ) %>%
      filter(
        .by = c(cruise_id, station, depth.x),
        time_diff == min(time_diff)
      ) %>%
      select(-time_diff)

    
    write_csv(
      env_cmtx,
      file = here("data", "processed", 
                   glue("cmtx_env_data_", 
                        "{Sys.Date()}",
                        ".csv")
                  )
    )
    }
```

